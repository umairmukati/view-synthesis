{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload, import_module\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pdb\n",
    "from PIL import Image as im\n",
    "import _pickle as pickle\n",
    "\n",
    "\n",
    "from functions import MyDataset, customTransform, get_variable, get_numpy, compute_gradient, psnr_1, sd_to_usd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASET PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    dataset_file = r\"C:\\Users\\mummu\\Documents\\Datasets\\srinivasan\\trainset\\h5\\8bit.h5\"\n",
    "    test_file    = r\"C:\\Users\\mummu\\Documents\\Datasets\\srinivasan\\testset\\h5\\8bit.h5\"\n",
    "    model_file   = r\"model\\model.pt\"\n",
    "    network_file = r\"network\"\n",
    "    trainwr_file = r\"runs\\train\"\n",
    "    testwr_file  = r\"runs\\test\"\n",
    "elif os.name == 'posix':\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASIC PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size     = 192\n",
    "batch_size     = 300\n",
    "minibatch_size = 8\n",
    "gamma_val      = 0.4\n",
    "lfsize         = [372, 540, 7, 7]\n",
    "num_workers    = 0\n",
    "num_test       = 10\n",
    "num_minibatch  = batch_size//minibatch_size\n",
    "batch_affine   = True\n",
    "num_epochs     = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INITIALIZE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                     transforms.Lambda(customTransform)])\n",
    "\n",
    "train_dataset  = MyDataset(dataset_file, lfsize, data_transform)\n",
    "test_dataset   = MyDataset(test_file, lfsize, data_transform)\n",
    "\n",
    "train_loader   = torch.utils.data.DataLoader(train_dataset, batch_size=minibatch_size, num_workers=num_workers, shuffle=True)\n",
    "test_loader    = torch.utils.data.DataLoader(train_dataset, batch_size=minibatch_size, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOOKING FOR SAVED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##converting network to cuda-enabled\n",
      "Model successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "network_module = import_module(network_file)\n",
    "reload(network_module)\n",
    "Net = network_module.Net\n",
    "\n",
    "net = Net((patch_size, patch_size), minibatch_size, lfsize, batchAffine=batch_affine)\n",
    "if torch.cuda.is_available():\n",
    "    print('##converting network to cuda-enabled')\n",
    "    net.cuda()\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(model_file)\n",
    "    \n",
    "    epoch_id = checkpoint['epoch']\n",
    "    net.load_state_dict(checkpoint['model'].state_dict())\n",
    "    print('Model successfully loaded.')\n",
    "    \n",
    "except:\n",
    "    print('No model.')\n",
    "    epoch_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def count_parameters(model): return list(name for name, param in model.named_parameters() if param.requires_grad == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1256649"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.L1Loss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    costs = []\n",
    "    psnr_vec = []\n",
    "            \n",
    "    for batch_num in range(num_minibatch):\n",
    "        \n",
    "        # fetching training batch\n",
    "        corners, pers, ind = next(iter(train_loader))\n",
    "        \n",
    "        # converting to trainable variables\n",
    "        X_corners = get_variable(corners)\n",
    "        T_view = get_variable(pers)\n",
    "        p = get_variable(ind[:,0])\n",
    "        q = get_variable(ind[:,-1])\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        O_view, _, _ = net(X_corners, p, q)\n",
    "        \n",
    "        # Computing batch loss\n",
    "        batch_loss = criterion1(O_view, T_view) + .5*criterion2(compute_gradient(O_view),\n",
    "                                                          compute_gradient(T_view))\n",
    "        \n",
    "        # Backpropagation\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # recording performance\n",
    "        costs.append(get_numpy(batch_loss))\n",
    "        net_out = get_numpy(O_view)\n",
    "        Y = get_numpy(T_view)      \n",
    "        psnr_vec.append([psnr_1(sd_to_usd(np.squeeze(net_out[i])), sd_to_usd(np.squeeze(Y[i]))) for i in range(minibatch_size)])\n",
    "    \n",
    "        \n",
    "    return np.mean(costs), np.mean(psnr_vec)\n",
    "\n",
    "def eval_epoch():\n",
    "    costs = []\n",
    "    psnr_vec = []\n",
    "    \n",
    "    for batch_num in range(num_test):\n",
    "        \n",
    "        # fetching training batch\n",
    "        corners, pers, ind = next(iter(test_loader))\n",
    "        \n",
    "        # converting to trainable variables\n",
    "        X_corners = get_variable(corners)\n",
    "        T_view = get_variable(pers)\n",
    "        p = get_variable(ind[:,0])\n",
    "        q = get_variable(ind[:,-1])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            O_view, _, _ = net(X_corners, p, q)\n",
    "            \n",
    "            # Computing batch loss\n",
    "            batch_loss = criterion1(O_view, T_view) + .5*criterion2(compute_gradient(O_view), compute_gradient(T_view))\n",
    "            \n",
    "            # recording performance\n",
    "            costs.append(get_numpy(batch_loss))\n",
    "            net_out = get_numpy(O_view)\n",
    "            Y = get_numpy(T_view)\n",
    "            psnr_vec.append([psnr_1(sd_to_usd(np.squeeze(net_out[i])), sd_to_usd(np.squeeze(Y[i]))) for i in range(minibatch_size)])\n",
    "\n",
    "    return np.mean(costs), np.mean(psnr_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mummu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2018:\n",
      "Epoch 2018, train_cost 0.037, psnr 4.1e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mummu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\mummu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\mummu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AvgPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\mummu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GroupNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2019:\n",
      "Epoch 2019, train_cost 0.041, psnr 4e+01\n",
      "Epoch 2020:\n",
      "Epoch 2020, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2021:\n",
      "Epoch 2021, train_cost 0.04, psnr 4e+01\n",
      "Epoch 2022:\n",
      "Epoch 2022, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2023:\n",
      "Epoch 2023, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2024:\n",
      "Epoch 2024, train_cost 0.04, psnr 4e+01\n",
      "Epoch 2025:\n",
      "Epoch 2025, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2026:\n",
      "Epoch 2026, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2027:\n",
      "Epoch 2027, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2028:\n",
      "Epoch 2028, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2029:\n",
      "Epoch 2029, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2030:\n",
      "Epoch 2030, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2031:\n",
      "Epoch 2031, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2032:\n",
      "Epoch 2032, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2033:\n",
      "Epoch 2033, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2034:\n",
      "Epoch 2034, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2035:\n",
      "Epoch 2035, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2036:\n",
      "Epoch 2036, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2037:\n",
      "Epoch 2037, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2038:\n",
      "Epoch 2038, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2039:\n",
      "Epoch 2039, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2040:\n",
      "Epoch 2040, train_cost 0.036, psnr 4.1e+01\n",
      "Epoch 2041:\n",
      "Epoch 2041, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2042:\n",
      "Epoch 2042, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2043:\n",
      "Epoch 2043, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2044:\n",
      "Epoch 2044, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2045:\n",
      "Epoch 2045, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2046:\n",
      "Epoch 2046, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2047:\n",
      "Epoch 2047, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2048:\n",
      "Epoch 2048, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2049:\n",
      "Epoch 2049, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2050:\n",
      "Epoch 2050, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2051:\n",
      "Epoch 2051, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2052:\n",
      "Epoch 2052, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2053:\n",
      "Epoch 2053, train_cost 0.036, psnr 4.1e+01\n",
      "Epoch 2054:\n",
      "Epoch 2054, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2055:\n",
      "Epoch 2055, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2056:\n",
      "Epoch 2056, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2057:\n",
      "Epoch 2057, train_cost 0.036, psnr 4.1e+01\n",
      "Epoch 2058:\n",
      "Epoch 2058, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2059:\n",
      "Epoch 2059, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2060:\n",
      "Epoch 2060, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2061:\n",
      "Epoch 2061, train_cost 0.036, psnr 4.1e+01\n",
      "Epoch 2062:\n",
      "Epoch 2062, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2063:\n",
      "Epoch 2063, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2064:\n",
      "Epoch 2064, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2065:\n",
      "Epoch 2065, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2066:\n",
      "Epoch 2066, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2067:\n",
      "Epoch 2067, train_cost 0.035, psnr 4.1e+01\n",
      "Epoch 2068:\n",
      "Epoch 2068, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2069:\n",
      "Epoch 2069, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2070:\n",
      "Epoch 2070, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2071:\n",
      "Epoch 2071, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2072:\n",
      "Epoch 2072, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2073:\n",
      "Epoch 2073, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2074:\n",
      "Epoch 2074, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2075:\n",
      "Epoch 2075, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2076:\n",
      "Epoch 2076, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2077:\n",
      "Epoch 2077, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2078:\n",
      "Epoch 2078, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2079:\n",
      "Epoch 2079, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2080:\n",
      "Epoch 2080, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2081:\n",
      "Epoch 2081, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2082:\n",
      "Epoch 2082, train_cost 0.036, psnr 4.1e+01\n",
      "Epoch 2083:\n",
      "Epoch 2083, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2084:\n",
      "Epoch 2084, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2085:\n",
      "Epoch 2085, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2086:\n",
      "Epoch 2086, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2087:\n",
      "Epoch 2087, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2088:\n",
      "Epoch 2088, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2089:\n",
      "Epoch 2089, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2090:\n",
      "Epoch 2090, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2091:\n",
      "Epoch 2091, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2092:\n",
      "Epoch 2092, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2093:\n",
      "Epoch 2093, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2094:\n",
      "Epoch 2094, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2095:\n",
      "Epoch 2095, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2096:\n",
      "Epoch 2096, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2097:\n",
      "Epoch 2097, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2098:\n",
      "Epoch 2098, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2099:\n",
      "Epoch 2099, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2100:\n",
      "Epoch 2100, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2101:\n",
      "Epoch 2101, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2102:\n",
      "Epoch 2102, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2103:\n",
      "Epoch 2103, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2104:\n",
      "Epoch 2104, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2105:\n",
      "Epoch 2105, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2106:\n",
      "Epoch 2106, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2107:\n",
      "Epoch 2107, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2108:\n",
      "Epoch 2108, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2109:\n",
      "Epoch 2109, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2110:\n",
      "Epoch 2110, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2111:\n",
      "Epoch 2111, train_cost 0.04, psnr 4e+01\n",
      "Epoch 2112:\n",
      "Epoch 2112, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2113:\n",
      "Epoch 2113, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2114:\n",
      "Epoch 2114, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2115:\n",
      "Epoch 2115, train_cost 0.04, psnr 4e+01\n",
      "Epoch 2116:\n",
      "Epoch 2116, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2117:\n",
      "Epoch 2117, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2118:\n",
      "Epoch 2118, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2119:\n",
      "Epoch 2119, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2120:\n",
      "Epoch 2120, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2121:\n",
      "Epoch 2121, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2122:\n",
      "Epoch 2122, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2123:\n",
      "Epoch 2123, train_cost 0.041, psnr 4e+01\n",
      "Epoch 2124:\n",
      "Epoch 2124, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2125:\n",
      "Epoch 2125, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2126:\n",
      "Epoch 2126, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2127:\n",
      "Epoch 2127, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2128:\n",
      "Epoch 2128, train_cost 0.04, psnr 4e+01\n",
      "Epoch 2129:\n",
      "Epoch 2129, train_cost 0.04, psnr 4e+01\n",
      "Epoch 2130:\n",
      "Epoch 2130, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2131:\n",
      "Epoch 2131, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2132:\n",
      "Epoch 2132, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2133:\n",
      "Epoch 2133, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2134:\n",
      "Epoch 2134, train_cost 0.036, psnr 4.1e+01\n",
      "Epoch 2135:\n",
      "Epoch 2135, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2136:\n",
      "Epoch 2136, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2137:\n",
      "Epoch 2137, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2138:\n",
      "Epoch 2138, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2139:\n",
      "Epoch 2139, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2140:\n",
      "Epoch 2140, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2141:\n",
      "Epoch 2141, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2142:\n",
      "Epoch 2142, train_cost 0.036, psnr 4.1e+01\n",
      "Epoch 2143:\n",
      "Epoch 2143, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2144:\n",
      "Epoch 2144, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2145:\n",
      "Epoch 2145, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2146:\n",
      "Epoch 2146, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2147:\n",
      "Epoch 2147, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2148:\n",
      "Epoch 2148, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2149:\n",
      "Epoch 2149, train_cost 0.035, psnr 4.1e+01\n",
      "Epoch 2150:\n",
      "Epoch 2150, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2151:\n",
      "Epoch 2151, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2152:\n",
      "Epoch 2152, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2153:\n",
      "Epoch 2153, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2154:\n",
      "Epoch 2154, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2155:\n",
      "Epoch 2155, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2156:\n",
      "Epoch 2156, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2157:\n",
      "Epoch 2157, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2158:\n",
      "Epoch 2158, train_cost 0.041, psnr 4e+01\n",
      "Epoch 2159:\n",
      "Epoch 2159, train_cost 0.041, psnr 4.1e+01\n",
      "Epoch 2160:\n",
      "Epoch 2160, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2161:\n",
      "Epoch 2161, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2162:\n",
      "Epoch 2162, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2163:\n",
      "Epoch 2163, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2164:\n",
      "Epoch 2164, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2165:\n",
      "Epoch 2165, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2166:\n",
      "Epoch 2166, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2167:\n",
      "Epoch 2167, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2168:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2168, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2169:\n",
      "Epoch 2169, train_cost 0.036, psnr 4.1e+01\n",
      "Epoch 2170:\n",
      "Epoch 2170, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2171:\n",
      "Epoch 2171, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2172:\n",
      "Epoch 2172, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2173:\n",
      "Epoch 2173, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2174:\n",
      "Epoch 2174, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2175:\n",
      "Epoch 2175, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2176:\n",
      "Epoch 2176, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2177:\n",
      "Epoch 2177, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2178:\n",
      "Epoch 2178, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2179:\n",
      "Epoch 2179, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2180:\n",
      "Epoch 2180, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2181:\n",
      "Epoch 2181, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2182:\n",
      "Epoch 2182, train_cost 0.036, psnr 4.1e+01\n",
      "Epoch 2183:\n",
      "Epoch 2183, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2184:\n",
      "Epoch 2184, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2185:\n",
      "Epoch 2185, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2186:\n",
      "Epoch 2186, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2187:\n",
      "Epoch 2187, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2188:\n",
      "Epoch 2188, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2189:\n",
      "Epoch 2189, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2190:\n",
      "Epoch 2190, train_cost 0.04, psnr 4e+01\n",
      "Epoch 2191:\n",
      "Epoch 2191, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2192:\n",
      "Epoch 2192, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2193:\n",
      "Epoch 2193, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2194:\n",
      "Epoch 2194, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2195:\n",
      "Epoch 2195, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2196:\n",
      "Epoch 2196, train_cost 0.04, psnr 4e+01\n",
      "Epoch 2197:\n",
      "Epoch 2197, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2198:\n",
      "Epoch 2198, train_cost 0.04, psnr 4e+01\n",
      "Epoch 2199:\n",
      "Epoch 2199, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2200:\n",
      "Epoch 2200, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2201:\n",
      "Epoch 2201, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2202:\n",
      "Epoch 2202, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2203:\n",
      "Epoch 2203, train_cost 0.04, psnr 4e+01\n",
      "Epoch 2204:\n",
      "Epoch 2204, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2205:\n",
      "Epoch 2205, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2206:\n",
      "Epoch 2206, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2207:\n",
      "Epoch 2207, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2208:\n",
      "Epoch 2208, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2209:\n",
      "Epoch 2209, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2210:\n",
      "Epoch 2210, train_cost 0.04, psnr 4e+01\n",
      "Epoch 2211:\n",
      "Epoch 2211, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2212:\n",
      "Epoch 2212, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2213:\n",
      "Epoch 2213, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2214:\n",
      "Epoch 2214, train_cost 0.036, psnr 4.1e+01\n",
      "Epoch 2215:\n",
      "Epoch 2215, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2216:\n",
      "Epoch 2216, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2217:\n",
      "Epoch 2217, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2218:\n",
      "Epoch 2218, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2219:\n",
      "Epoch 2219, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2220:\n",
      "Epoch 2220, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2221:\n",
      "Epoch 2221, train_cost 0.042, psnr 4.1e+01\n",
      "Epoch 2222:\n",
      "Epoch 2222, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2223:\n",
      "Epoch 2223, train_cost 0.037, psnr 4.1e+01\n",
      "Epoch 2224:\n",
      "Epoch 2224, train_cost 0.039, psnr 4.1e+01\n",
      "Epoch 2225:\n",
      "Epoch 2225, train_cost 0.04, psnr 4.1e+01\n",
      "Epoch 2226:\n",
      "Epoch 2226, train_cost 0.038, psnr 4.1e+01\n",
      "Epoch 2227:\n",
      "Epoch 2227, train_cost 0.037, psnr 4.1e+01\n"
     ]
    }
   ],
   "source": [
    "valid_accs, train_accs, test_accs = [], [], []\n",
    "\n",
    "writer_train = SummaryWriter(trainwr_file)\n",
    "writer_test  = SummaryWriter(testwr_file)\n",
    "\n",
    "while epoch_id < num_epochs:\n",
    "    epoch_id += 1\n",
    "    \n",
    "    try:   \n",
    "        net.train()\n",
    "        train_cost, train_psnr = train_epoch()\n",
    "        \n",
    "        net.eval()\n",
    "        test_cost, test_psnr = eval_epoch()\n",
    "        \n",
    "        print(\"Epoch %d:\" % epoch_id)     \n",
    "        print(\"Epoch {0:0}, train_cost {1:.2}, psnr {2:.2}\".format(epoch_id, train_cost, train_psnr))\n",
    "        \n",
    "        writer_train.add_scalar('psnr', train_psnr, epoch_id)\n",
    "        writer_train.add_scalar('loss', train_cost, epoch_id)\n",
    "        writer_test.add_scalar('psnr', test_psnr, epoch_id)\n",
    "        writer_test.add_scalar('loss', test_cost, epoch_id)\n",
    "        \n",
    "        torch.save({'model': net, 'epoch': epoch_id}, model_file)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print('\\nKeyboardInterrupt')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
